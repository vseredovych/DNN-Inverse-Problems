% ============================================ %

\documentclass[14pt,a4paper]{extarticle}
%\documentclass[12pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[ukrainian]{babel}


\usepackage{amssymb}
\usepackage{physics}
\usepackage{amsmath}
% \operatorname*{argmin}_\theta f(x)
% \operatorname*{arg\,max}_\theta f(x)



\usepackage[active]{srcltx}
\usepackage[final]{pdfpages}

\usepackage[hidelinks]{hyperref}

\usepackage{verbatim}

% ============================================ %

%\pagestyle{empty}                     %нумерацiя сторiнок i т.д.
\pagestyle{headings}                   %нумерацiя сторiнок вгорi зправа i т.д.
%\renewcommand{\baselinestretch}{1.5}   %мiжстрiчковий інтервал
%\parindent=7.5mm                      %абзацний відступ
\righthyphenmin=2                     %перенос 2 останніх букв
\pagenumbering{arabic}
\tolerance=400
\mathsurround=2pt
\hfuzz=1.5pt

% ============================================ %

\hoffset=-0.5cm        %+2.5cm -- вiдступ вiд лiвого краю
\voffset=-1.5cm        %+2.5cm -- вiдступ зверху
\oddsidemargin=0.1cm   %ліве поле
\topmargin=0.1cm       %верхнє поле
\headheight=0.5cm      %висота верхнього колонтитулу
\footskip=1cm          %висота нижнього колонтитулу
\headsep=0.3cm         %відступ від колонт. до тексту
\textwidth=17cm        %ширина сторінки
\textheight=25.5cm     %висота сторінки

% ============================================ %
	
\newcounter{e}
\setcounter{e}{0}
\newcommand{\n}{\refstepcounter{e} (\arabic{e})}

\newcounter{pic}
\setcounter{pic}{0}
\newcommand{\pic}[1]{\refstepcounter{pic} \vspace{-0.3cm}\textit{Рисунок \arabic{pic}\label{#1}.}}

\newcounter{tabl}
\setcounter{tabl}{0}
\newcommand{\tabl}[1]{\refstepcounter{tabl} \vspace{-0.3cm}\textit{Таблиця \arabic{tabl}\label{#1}.}}

\newcounter{dod}
\setcounter{dod}{0}
\newcommand{\dod}[1]{\refstepcounter{dod} \textit{Додаток \arabic{dod}\label{#1}.}}


\newtheorem{theorem}{Теорема}[section]
\newtheorem{defn}[theorem]{Означення}
\newtheorem{lemma}[theorem]{Лема}

\newcommand{\proof}{\textit{Доведення. \space}}
% \setcounter{page}{1}
% \setcounter{section}{1}

\numberwithin{equation}{section}
\numberwithin{figure}{section}


% \newcommand{\unknownx}{	\boldsymbol{$1}^{\star}}
% \newcommand{\bt}[1]{\textbf{#1}}
% custom commands
\newcommand{\tran}{^{T}}
\newcommand{\ith}{^{(i)}}
\newcommand{\lth}{^{(l)}}


% ============================================ %
	
% bibliography
\usepackage[
	backend=biber,
	style=numeric,
	sorting=none
]{biblatex}
\addbibresource{resources/bibliography.bibtex}

% ============================================ %

\begin{document}
	% ============================================ %
	\begin{titlepage}%
		\begin{center}
			{\textbf{ЛЬВІВСЬКИЙ НАЦІОНАЛЬНИЙ УНІВЕРСИТЕТ \\ ІМЕНІ ІВАНА ФРАНКА}}\par
			{Факультет прикладної математики та інформатики \\ Кафедра обчислювальної математики}\par
			\begin{center}
				
			\end{center}
			\vspace{25mm}
			{\textbf{\huge{Курсова робота}}}\par
			\vspace{5mm}
			{\large{Використання глибокого навчання для обернених задач}}\par
			\vspace{5mm}
			{}\par %subtitle
		\end{center}
		
		\vfill
		\vskip80pt
		
		\begin{flushleft}
			\hskip 8cm 
			Виконав студент IV курсу групи
			\\ \hskip8cm
			ПМп-41 напрямку підготовки 
			\\ \hskip8cm
			(спеціальності)
			\\ \hskip8cm
			113 -- ``Прикладна математика''
			\\ \hskip8cm
			Середович В.B.
		\end{flushleft}
		\begin{flushleft}
			\hskip8cm 
			Курівник: Музичук Ю.А.
		\end{flushleft}
		
		\vfill
		
		\begin{center}
			\large
			Львів - 2021
		\end{center}
	\end{titlepage}

	% ============================================ %
	% Зміст
	\addtocontents{toc}{\protect\thispagestyle{empty}}
	\tableofcontents

	% ============================================ %
	% Вступ
	
	\newpage
	\thispagestyle{empty}
	\addcontentsline{toc}{section}{Вступ}
	\section*{Вступ}
	TODO: Вступ про типи обернені задач та глибоке навчання
	\\
	Оберненими задачами називають такі задачі, коли необхідно відновити параметри які характеризують деяку модель з використанням непрямих спостережень. До них можна віднести багото по відновленню зображень зменшення кількості шуму (deblurring) чи заповнення втрачених даних (inpainting).

	\begin{comment}
	"""
	http://repository.dnu.dp.ua:1100/upload/ddcba97cf65d7af08acc8b934080e148.pdf
	
	1) визначати механічні та теплофізичні властивості матеріалів, ідентифікув ати полімерні і композитні матеріали, біоматеріали, п'єзокерам ічні матеріали;
	2) розв’язувати задачі сейсморозвідки , а саме визначати розташування й
	потужності покладів корисних копалин за відбитими від родовища звуковими сиг
	налами;
	3) розв’язувати проблеми неруйнівного контролю, а саме визначати розташування й конфігурацію дефекту за виміряним на поверхні тіла полем пружних
	переміщень або за резонансними частотами;
	4) моделювати явища акустичної емісії та встановлювати зв'язок між осн ов
	ними характеристиками емісії та хар актеристиками напруженого стану, д ослід
	ження цього явища дозволяє виявити стан конструкції, що передує її руйнуванню;
	5) розв’язувати задачі рентгенівської й акустичної томографії.
	Вибираючи математичну модель, звичайно фіксують два етапи:
	1) вибір структури оператора A, що здійснює відображення входу xt на
	вихід yt;
	""'"
	content...
	\end{comment}


	% ============================================ %
	
	\newpage
	\thispagestyle{empty}
	\section{Постановка задачі} 
	
	
	\begin{comment}
	"""
	Відповідно до поняття, уведеного на початку століття Ж. Адамаром, задачу
	z  Ru називають коректно поставленою, якщо вона задовольняє тр и умови:
	1) за кожного u U розв'язок задачі існує;
	2) розв'язок є єдиний за кожного u U ;
	3) розв'язок є стійкий до малих варіацій величини u , тобто достатньо малим
	зміненням величини u відповідають як завгодно малі зміни величини z [1, 7].
	Якщо задача не задовольняє хоча б одну із зазначених умов, то її називають некоректно поставленою.
	Очевидно тепер, що обернені задачі в розглянутих прикладах відносять до
	числа некоректно поставлених, оскільки в них порушується третя, а мо жливо, і
	перша із зазначених вище умов. Некоректність постановки обернен ої задачі і є її
	математична особливість. Якщо для пошуку наближеного розв’язку оберненої з адачі застосовувати будь-який класичний алгоритм формально, не враховуючи н екоректність постановки задачі, то є великий ризик отримати результат, який не
	має ні наукової, ні прикладної цінності.
	"""
	\end{comment}
	
	
	
	
	Оберненими задачами будемо вважати такі задачі, в яких невідомим є $n-$ піксельне зображення $\boldsymbol{x} \in \mathbb{R}^{n}$ яке було отримане з $m$ вимірювань $\boldsymbol{y} \in \mathbb{R}^{m}$ відповідно до рівняння
	\begin{equation}
	\label{forward-problem}
	\boldsymbol{y}=\mathcal{A}\left(\boldsymbol{x}\right)+\boldsymbol{\varepsilon}
	\end{equation}
	де $\mathcal{A}$ - це прямий оператор вимірювання та $\boldsymbol{\varepsilon}$ є певним вектором шуму. Метою задачі є відновлення $x$ з $y$. Можна розглянути більш загальний випадок моделі неадитивного шуму, який має вигляд 
	\begin{equation}
	\label{forward-problem-non-additive}
	\boldsymbol{y}=\mathcal{N}\left(\mathcal{A}\left(\boldsymbol{x}\right)\right)
	\end{equation}
	де $\mathcal{N}(\cdot)$ є прикладами вибірки з шумом.


	\begin{defn}
		\label{well-posed}
		Відповідно до поняття, уведеного Жаком Адамаром, задачу \ref{forward-problem-non-additive} називають коректно поставленою, якщо вона задовольняє наступні умови: 
		\begin{enumerate}
			\item Для кожного $x$ розв'язок задачі існує.
			\item Розв'язок є єдиний для кожного $x$.
			\item Розв'язок є стійкий до малих варіацій величини $x$, тобто достатньо малим зміненням величини $x$ відповідають як завгодно малі зміни величини $y$.
		\end{enumerate}
	\end{defn}

	\begin{defn}
		\label{ill-posed}	
		Задачу, яка не задовільняє хоча б одну із умов означення \ref{well-posed}, називають некоректно поставленою.
	\end{defn}

	Отже, очевидно, що розглянута обернена задача є некоректно (або погано обумовленою), оскільки в ній порушуються умови означення \ref{well-posed}. Така задача знаходження єдиного розв'язку, яка задовілняє спостереженням є складною або неможливою, за умови відсутності попередніх знаннь про дані.

	Оцінку справжнього зображення $\boldsymbol{x}$ з $\boldsymbol{y}$  вимірювання називають задачею реконструкції зображення. Класичні підподи до реконструкції зображеннь припускають наявність деякої попередньої інформації про зображення, яку називають пріором. В якості пріору можуть виступати параметри гладкості, щільності та інші геометричні властивості зображення.

	%TODO
	Отже метою даної роботи буде розв'язання таких обернених задач за допомогую алгоритмів глибокого навчання. Зокрема, будемо розглядати проблему зменшення кількості шуму у зобреженнях.

	%	Метою даної роботи є дослідження ефективності різних методів атак на лінійні моделі машинного навчання, та аналіз можливих методів захисту від них.
		
	%	Виходячи з мети, визначеними завданнями роботи є:
	%	\begin{itemize}
	%		\item Реалізувати лінійну модель машинного навчання
	%		\item Розглянути різні методи генерування змагальних прикладів
	%		\item Застосувати атаки на створену модель та проаналізувати їх ефективність
	%		\item Розглянути можливі методи захисту від атак
	%	\end{itemize}
		
	% ============================================ %
	\newpage
	\thispagestyle{empty}
	\section{Структура обернених задач}

	%\subsection{Розв'язування обернених задач}
	
	Якщо розподіл шуму відомий, $x$ можна відновити розвязавши задачу оцінки максимальної ймовірності (Maximum Likelihood):
	$$
	\hat{\boldsymbol{x}}_{\mathrm{ML}}
	=\underset{\boldsymbol{x}}{\arg \max{ p (\boldsymbol{y} \mid \boldsymbol{x}) }}
	=\underset{\boldsymbol{x}}{\arg \min }-\log p(\boldsymbol{y} \mid \boldsymbol{x})
	$$
	де $p(\boldsymbol{y} \mid \boldsymbol{x})$ це ймовірність спостереження $\boldsymbol{y}$ за умови якщо $\boldsymbol{x}$ є справжнім зображенням.
	
	В залежності від умов задачі, можуть бути відомі попередні дані про те яким має бути $x$. Ці умови можуть бути використанні для формування  задачі оцінки максимальної апостеріорної ймовірності (maximum a posteriori), що приводить до задачі
	$$
	\hat{\boldsymbol{x}}_{\mathrm{MAP}}
	=\underset{\boldsymbol{x}}{\arg \max{ p(\boldsymbol{x} \mid \boldsymbol{y}) }}
	=\underset{\boldsymbol{x}}{\arg -\max{ p(\boldsymbol{y} \mid \boldsymbol{x})} } p(\boldsymbol{x})
	=\underset{\boldsymbol{x}}{\arg \min }-\ln p(\boldsymbol{y} \mid \boldsymbol{x})-\ln p(\boldsymbol{x})
	$$
	Для випадку білого гаусівського шуму, формулювання MAP дає:
	\begin{equation}
	\label{MAP-avgn}
	 \frac{1}{2}\|\mathcal{A}(\boldsymbol{x})-\boldsymbol{y}\|_{2}^{2}+r(\boldsymbol{x})
	\end{equation}
	де  $r(\boldsymbol{x})$ є пропорційним до негативного логарифмічного пріора $\boldsymbol{x} .$. Прикладами такого підходу є регуляризація Тіхонова. 
	
	
	\begin{comment}
	Його метод оснований на залучені додаткової
	інформації про розв’язок, яка може бути як якісною так і кількісною.

	Наприклад, можна шукати розв’язок максимально близький до деякого
	профілю, тобто, до декого вектора $y$.	
	[0
	. Концепція регуляризації зводиться до
	заміни початкової некоректної задачі на задачу про мінімізацію наступної
	функції: 0 W([,l) = A[ - B + l [ -[ , де l - малий додатній параметр
	регуляризації, який необхідно підібрати певним способом. Якщо розглядати
	не дискретну, а неперервну задачу, тоді W([,l), буде представляти собою не
	функцію, а функціонал, який має назву функціонал Тіхонова.
	%$$
	%\underset{x \in \mathbb{R}^{n}}{\min }
	%\left\{\|Ax-b\|_{2}^{2}+ \lambda \|x\|_{2}^{2}\right\}
	%$$
	\end{comment}

	Задача маскимальної апостеріорної оцінки може використовуватись для реконструкції зображень, однак такий такий підхід може бути не таким ефективним, якщо розподіл шуму або прямий оператор є невідомі. 
	Алгоритми основані на використанні машинного навчання дають змогу побороти більшість з цих труднощів, що робить їх ефективною альтернативою класичному підходу.

	\section{Огляд машинного навчання для розв'язування обернених задач}

	\subsection{Контрольоване і некортнольоване навчання}
	Перший і найпоширеніший тип розв'язування оберених задач з викорисатнням глибогоко навчання є контрольована інверсія. Ідея полягає у створенні співвідношення між датасетом справжніх зображень $x$ та відповідними вимірюваннями $y$. Тобто ми можемо натренувати нейронну мережу приймати значення $y$ та реконстроювати оберенне значення $x$. Цей підхід є дуже ефективним, однак є чутливим до змін в опереторі вимірювання $A$. 
	
	Другим типом розв'язування обернених задач є неконтрольованого навчання. Він передбачає, що інформація про пари вхідної та вихідної інформації $x$ та $y$ невідомі під час тренування. До нього можна віднести ситуації коли відомі тільки справжні зображення $x$ або тільки результати вимірювання $y$.

	Ці два підходи мають фундоментальні відміності і ця робота націлена саме на методи контрольованого навчання, тому що очікується, що вони дадуть кращі результати в порівнянні з класичними методами. 
	
	\subsection{Класифікація навчання розв'язування обернених задач}
	
	TODO
	
	% ============================================ %
	\newpage
	\thispagestyle{empty}
	\section{Методи глибокого навчання для обернених задач}
	
	\subsection{Autoencoder}		
	Автоенкодером називають нейрону мережу яка навчається копіювати свої вхідні дані у вихідні. Така мережа має проміжний шар $h$, також відомий як латентний, який зберігає параметри необхідні для представлення вхідних даних. Таку нейрону мережу можна подати у складі двох частин: функції енкодера $h = f(x)$ та декодера який відтворює $r = g(h)$. Якщо автоенкодеру вдається навчитися просто відтворювати $g(f(x)) = x$ для всіх прикладів, то це не має особливої користі. Тому автоенкодери зазвичай обмежують таким чином щоб вони не могли відтворювати ідеальну копію вхідних даним.
	
	\begin{comment}
	Usually they arerestricted in ways that allow them to copy only approximately, and to copy onlyinput that resembles the training data. Because the model is forced to prioritizewhich aspects of the input should be copied, it often learns useful properties of thedata
		
	Rather than adding a penalty $\Omega$ to the cost function, we can obtain an autoencoder that learns something useful by changing the reconstruction error term of the cost function.
	\end{comment}
	
	\subsection{Denoising autoencoder (DAE)}
	Класичні автоенкодери мінімізують деяку функцію:
	$$
	L(\boldsymbol{x}, g(f(\boldsymbol{x})))
	$$
	ду $L$ це штрафна функція яка карає $g(f(\boldsymbol{x}))$ за відмінність від $\boldsymbol{x}$, таку як $L^{2}$ норму від їх різниці. Це призводить до того, що композиція функцій $g \circ f$ навчається бути функцією тотожнього відобрадення якщо для того є можливість. На відміну від цього, автоенкодер для видалення шуму, або denoising autoencoder (DAE) мінімізує
	$$
	L(\boldsymbol{x}, g(f(\tilde{\boldsymbol{x}})))
	$$
	де $\tilde{\boldsymbol{x}}$ є копією $\boldsymbol{x}$ який був пошкоджений деяким шумом. Отже, такий автоенкодер має відновити пошкодження, натомість простому відтвореню вхідних даних.

	TODO?
	Навчання видалення шуму змушує $f$ та $g$ явно вивчати структуру даних для 	
	
	\newpage
	\subsection{Модель глибокої нейронох мережі для автоенкодера}
	Нехай маємо набір тренувальних даних:
	\begin{equation*}
		(x^{(1)}, y^{(1)}), \quad (x^{(2)}, y^{(2)}), \quad ... \quad ,(x^{(m)}, y^{(m)})
	\end{equation*}
	\begin{equation*}
		\quad
		X =	
		\begin{bmatrix}
			\vdots  & \vdots  & & \vdots \\
			x^{(1)} & x^{(2)} &   \dots & x^{(m)}\\
			\vdots  & \vdots  & & \vdots
		\end{bmatrix}
		\quad
		x \in
		\begin{bmatrix}
			x_1   \\
			\dots \\
			x_n
		\end{bmatrix}
		\quad
	\end{equation*}
	\begin{equation*}
		\quad
		Y =	
		\begin{bmatrix}
			\vdots  & \vdots  & & \vdots \\
			y^{(1)} & y^{(2)} &   \dots & y^{(m)}\\
			\vdots  & \vdots  & & \vdots
		\end{bmatrix}
		\quad
		y \in
		\begin{bmatrix}
			y_1   \\
			\dots \\
			y_n
		\end{bmatrix}
		\quad
	\end{equation*}
	де
	\begin{itemize}
	\item $X$ - матриця в якій кожен $i-$ий стовпець є розгорнутим у вектор зображенням з деяким рівнем шум,  \newline $i=1,\dots,m$
	\item $Y$ - матриця в якії кожен $i-$ий стовпець є розгорнутим у вектором справжнього зображення
	\item $m$ - кількість прикладів
	\item $n$ - кількість характеристик в кожному прикладі (довжина розгорнутого в вектор зображення)
	\end{itemize}

	TODO : задача необхідно знайти...
	TODO: описати шари
	Для цього будемо використовувати алгоритм градієнтного спуску (ADAM?)
	необхідно мінімізувати ...

	\begin{comment}
	Активації ( a
	) – значення, що
	передаються від одного шару до іншого.
	На першому кроці це вхідні дані, на
	останньому – результат.
	\end{comment}
	
	Алгоритм прямого поширення
	\begin{equation}
		\label{dnn-forward-propagation}
		\begin{array}{l}
			\displaystyle
			z\lth=w^{(l)} a^{(l-1)}+b\lth
			\\[0.7cm]
			
			\displaystyle
			a\lth=\sigma (z\lth)
		\end{array}
	\end{equation}
	де
	\begin{itemize}
		\item $l$ - номер шару нейроної мережі, де $l = 1 \dotsc L$
		\item $n^{[l]}$ - кількість нейронів в $l$ шарі
		\item $a\lth$ - вектор стовбець з $n^{[l]}$ елементів (тобто $n^{[l]}$ $\cross$ 1 матриця), активація нейронів на для шару $l$
		\item $b\lth$ - вектор стовбець з $n^{[l]}$ елементів (тобто $n^{[l]}$ $\cross$ 1 матриця),  ваги зміщення
		\item $w\lth$ - $(n^{[l]} \cross n^{[l - 1]})$ матриця ваг поміж шарами $l-1$ та $l$
		\item $\sigma$ - це активаційна (стискуюча) функція, яку ми можемо прийняти як логістичну (для діапазону від 0 до 1) або tanh (для діапазону від -1 до 1), або будь-яку іншу диференційовану функцію.

	\end{itemize}
	
	Для кожного прикладу визначеми функцію витрат як середньо квадратичну похибку
	\begin{equation}
		\label{loss-function-mse}
		L(a^{(l, i)}, y\ith)  =  \frac{1}{2}  \| a^{(l, i)}  - y\ith \|_{L_2}^{2} = \frac{1}{2} \sum_{j=1}^{n}  (a^{(l, i)}_j -  y\ith_j)^2
	\end{equation}
	де $\| \cdot \|$ це евклідова відстань також відома як $L_2$ норма, $y$ - справжне зображення, $a$ - активація, l - номер шару
	
	На всії прикладах обчислюємо штрафну функцію
	\begin{equation}
		\label{cost-function}
		J(\omega, b) =  \frac{1}{m}  \sum_{i=1}^{m} L(a^{(L, i)}, y\ith)
	\end{equation}
	
	Задача полягає в тому щоб знайти параметри $w \in \mathbb{R}^{n_x}, b\in \mathbb{R}$ що мінімізують функцію $J(w, b)$.

	
	Далі обчислюємо похідну від функції середньо квадратичної похибки, останього шару нейронної мережі:
	\begin{equation*}
		\begin{array}{l}
			\displaystyle
			da^{(L, i)}
			=
			\pdv{L(a^{(L, i)}, y\ith)}{a^{(L, i)}}
			=
			\sum_{j=1}^{n}{(a^{(L, i)}_j - y\ith_j)}
			\\[0.7cm]

			\displaystyle
			dz^{(L, i)}
			=
			\pdv{L(a^{(L, i)}, y\ith)}{z^{(L, i)}}
			= 
			\pdv{L(a^{(L, i)}, y\ith)}{a^{(L, i)}} \pdv{a^{(L, i)}}{z^{(L, i)}}
			=
			\sum_{j=1}^{n}{(a^{(L, i)}_j - y\ith_j)} \sigma^{\prime}(z^{(L, i)});
			\\[0.7cm]

			\displaystyle
			dw^{(L, i)} 
			=
			\pdv{L(a^{(L, i)}, y\ith)}{w^{(L, i)}}
			=
			dz^{(L, i)} \pdv{z^{(L, i)}}{w^{(L, i)}}
			=
			dz^{(L, i)} a^{(L-1, i)};
			\\[0.7cm]
	
			\displaystyle
			db^{(L - 1, i)}
			=
			\pdv{L(a^{(L, i)}, y\ith)}{b^{(L, i)}}
			=
			dz^{(L, i)} \pdv{z^{(L, i)}}{b^{(L, i)}} 
			= 
			dz^{(L, i)}
			\\[0.7cm]
		\end{array}
	\end{equation*}

	


	\begin{comment}
			\displaystyle
			\pdv{a^{(L, i)}}{z^{(L, i)}}
			=
			\sigma^{\prime}(z^{(L, i)});
			\quad
			\pdv{z^{(L, i)}}{w^{(L-1, i)}}
			=
			a^{(L-1, i)};
			\quad
			\pdv{z^{(L, i)}}{b^{(L, i)}}
			=
			1;
	\end{comment}

	Перепишемо похідні в більш компактному вигляді:
	\begin{equation*}
		\begin{array}{l}
			\displaystyle
			\frac{\partial J(w, b)}{\partial W^{(l)}} =\frac{1}{m} \sum_{i}^{m} 	\delta^{(l, i)}\left(a^{(l-1, i)}\right)^{\top}
			\\[0.7cm]
	
			\displaystyle
			 \frac{\partial J(w, b)}{\partial b^{(l)}} =\frac{1}{m} \sum_{i}^{m} \delta^{(l, i)}
		\end{array}
	\end{equation*}

	\begin{equation*}
		\begin{array}{l}
		\displaystyle
			\delta^{(l, i)}
			=
			\left\{
			\begin{array}{l}
				\displaystyle
				\sum_{j=1}^{n}{ \left(a^{(L, i)}_j - y\ith_j\right)} \sigma^{\prime}(z^{(L, i)}), \quad \text{якщо } l = L
				\\[0.7cm]
				
				\displaystyle
				\left((W^{(l)})^{\top} \delta^{(l+1, i)} \right) \sigma^{\prime} (z^{(l, i)}), \quad \text{якщо } l < L
			\end{array}\right.
		\end{array}
	\end{equation*}

	В залежності від активаційної функції значення похідної для алгоритму зворотнього поширення буде відрізнятись. Розглянемо наступні варіанти активаційних функцій:	
	\begin{itemize}	
		\item \textbf{Sigmoid} (Logistic function)
		\label{sigmoid}
		\begin{equation}
			\begin{array}{l}
				\displaystyle
				a=\sigma(z)=\frac{1}{1+e^{-z}} \\[0.5cm]

				\displaystyle
				\sigma^{\prime}(z)=a(1-a)
			\end{array}
		\end{equation}
		
		\item \textbf{ReLU} (Rectiﬁed Linear Units)
		\begin{equation}
			\label{relu}
			\begin{array}{l}
				\displaystyle
				a=\sigma(z)=\max (0, z) \\[0.5cm]

				\displaystyle
				\sigma^{\prime}(z)=\left\{\begin{array}{l}
					0, z<0 \\
					1, z \geq 0
				\end{array}\right.
			\end{array}
		\end{equation}
		
		\item \textbf{Tanh} (Hyperbolic tangent)
		\begin{equation}
			\label{tanh}
			\begin{array}{l}
				\displaystyle
				a=\sigma(z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}} \\[0.5cm]

				\displaystyle
				\sigma^{\prime}(z)=1-a^{2}
			\end{array}
		\end{equation}
	\end{itemize}
	

	\begingroup
	\setlength{\abovedisplayskip}{0pt}
	\setlength{\belowdisplayskip}{0pt}
	

	
	
	
	\begin{comment}
	\begin{algorithm}
		\caption{Градієнтний спуск}
		\label{alg:gradient_descent}
		\begin{algorithmic}[1]
			\State \algorithmicrequire{Тренувальні дані $X, Y$, гіперпараметри: $N$, $\varepsilon$, $\alpha$}
			\State \algorithmicensure{ Оптимальні параметри моделі $w, b$}
			\State i = 0
			\While{ i $< N$ або ( $\|dw\| < \varepsilon$ та $\|db\| < \varepsilon$ )}
			\State $dw = \alpha \pdv{J(w, b)}{w}$
			\State $db = \alpha \pdv{J(w, b)}{b}$
			\State $\omega = \omega - dw$
			\State $b = b - db$
			\State i += 1
			\EndWhile
			\State \algorithmreturn{$\omega, b$}.
		\end{algorithmic}
	\end{algorithm}
	\end{comment}
	
	\begin{comment}
	\section{Датасет}
	В якості тренувального датасету будемо використовувати MNIST базу даних яка складається з 60 тис. тренувальних та 10 тис. тестувальних зображень рукописних цифр \ref{fig:minist_dataset}. Розмір кожного із них складає $28 \times 28$, а значення їх пікселів знаходяться в проміжку $[0, 255]$. На основі неї будемо здійснюватись тренування моделі і аналіз методів атак та захисту.
	\end{comment}


	% ============================================ %
	\newpage
	\thispagestyle{empty}
	\section{Реалізація та аналіз}
	
	\section{Герерація шуму}
	TODO
	
	\subsection{Реалізація}
	TODO

	\subsection{Аналіз}
	TODO
	
	% ============================================ %		
	\newpage
	\thispagestyle{empty}
	\section{Висновок}
	TODO
	
	%============================================ %
	\newpage
	\thispagestyle{empty}
	
	% Deep Learning Techniques for Inverse Problems in Imaging
	\nocite{ongie2020deep}
		
	% Solving ill-posed inverse problems using iterative deep neural networks
	\nocite{Adler_2017}
	\printbibliography[title={Бібліографія}]
	% ============================================ %
\end{document}