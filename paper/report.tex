% ============================================ %

\documentclass[14pt,a4paper]{extarticle}
%\documentclass[12pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[ukrainian]{babel}
\usepackage[active]{srcltx}
\usepackage[final]{pdfpages}
\usepackage[hidelinks]{hyperref}

\usepackage{verbatim}
\usepackage{amssymb}
\usepackage{physics}
\usepackage{amsmath}
\usepackage{algpseudocode}
\usepackage{algorithm}
\floatname{algorithm}{Algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input: }}
\renewcommand{\algorithmicensure}{\textbf{Output: }}
\newcommand{\algorithmreturn}{\textbf{return }}

% ============================================ %

%\pagestyle{empty}    %нумерацiя сторiнок i т.д.
\pagestyle{headings}    %нумерацiя сторiнок вгорi зправа i т.д.
%\renewcommand{\baselinestretch}{1.5}   %мiжстрiчковий інтервал
%\parindent=7.5mm                     %абзацний відступ
\righthyphenmin=2                     %перенос 2 останніх букв
\pagenumbering{arabic}
\tolerance=400
\mathsurround=2pt
\hfuzz=1.5pt

% ============================================ %

\hoffset=-0.5cm            %+2.5cm -- вiдступ вiд лiвого краю
\voffset=-1.5cm             %+2.5cm -- вiдступ зверху
\oddsidemargin=0.1cm  %ліве поле
\topmargin=0.1cm         %верхнє поле
\headheight=0.5cm       %висота верхнього колонтитулу
\footskip=1cm               %висота нижнього колонтитулу
\headsep=0.3cm            %відступ від колонт. до тексту
\textwidth=17cm            %ширина сторінки
\textheight=25.5cm      %висота сторінки

% ============================================ %
% custom commands

\newcounter{e}
\setcounter{e}{0}
\newcommand{\n}{\refstepcounter{e} (\arabic{e})}

\newcounter{pic}
\setcounter{pic}{0}
\newcommand{\pic}[1]{\refstepcounter{pic} \vspace{-0.3cm}\textit{Рисунок \arabic{pic}\label{#1}.}}

\newcounter{tabl}
\setcounter{tabl}{0}
\newcommand{\tabl}[1]{\refstepcounter{tabl} \vspace{-0.3cm}\textit{Таблиця \arabic{tabl}\label{#1}.}}

\newcounter{dod}
\setcounter{dod}{0}
\newcommand{\dod}[1]{\refstepcounter{dod} \textit{Додаток \arabic{dod}\label{#1}.}}

\newtheorem{theorem}{Теорема}[section]
\newtheorem{defn}[theorem]{Означення}
\newtheorem{lemma}[theorem]{Лема}

\newcommand{\proof}{\textit{Доведення. \space}}
\numberwithin{equation}{section}
\numberwithin{figure}{section}

\newcommand{\tran}{^{T}}
\newcommand{\ith}{^{(i)}}
\newcommand{\lth}{^{(l)}}

\newcommand{\tabboxl}[2]{\parbox{#1}{\vspace{0.1cm} #2 \vspace{0.1cm} }}

\newcommand{\tabboxr}[2]{\parbox{#1}{\vspace{-0.3cm}
		\begin{flushright} #2 \end{flushright} \vspace{-0.3cm} }}

\newcommand{\tabboxc}[2]{\parbox{#1}{\vspace{-0.3cm}
		\begin{center} #2 \end{center} \vspace{-0.3cm} }}

% \setcounter{page}{1}
% \setcounter{section}{1}
% ============================================ %
% bibliography
\usepackage[
	backend=biber,
	style=numeric,
	sorting=none
]{biblatex}
\addbibresource{../resources/bibliography.tex}

\nocite{ongie2020deep}
\nocite{Goodfellow-et-al-2016}
\nocite{Adler_2017}
\nocite{NIPS2012_6cdd60ea}
\nocite{10.5555/104279}
\nocite{1284395}
\nocite{boyat2015review}
\nocite{Dong_2019}
\nocite{DenoisingTikhonovTV}

% ============================================ %

\begin{document}
	% ============================================ %
	\thispagestyle{empty}
	\begin{center}
		{\textbf{ЛЬВІВСЬКИЙ НАЦІОНАЛЬНИЙ УНІВЕРСИТЕТ \\ ІМЕНІ ІВАНА ФРАНКА}}\par
		{Факультет прикладної математики та інформатики \\ Кафедра обчислювальної математики}\par
		\vspace{40mm}
		{\textbf{\huge{Курсова робота}}}\par
		\vspace{5mm}
		{\large{Використання глибокого навчання для обернених задач}}\par
		\vspace{5mm}\par %subtitle
	\end{center}
	
	\vfill
	\vskip80pt
	
	\begin{flushleft}
		\hskip 8cm 
		Виконав студент IV курсу групи
		\\ \hskip8cm
		ПМп-41 напрямку підготовки 
		\\ \hskip8cm
		(спеціальності)
		\\ \hskip8cm
		113 -- ``Прикладна математика''
		\\ \hskip8cm
		Середович В.B.
	\end{flushleft}
	\begin{flushleft}
		\hskip8cm 
		Керівник: Музичук Ю.А.
	\end{flushleft}
	
	\vfill
	
	\begin{center}
		\large
		Львів - 2021
	\end{center}


	% ============================================ %
	% Зміст
	\newpage
	\thispagestyle{empty}
	%\addtocontents{toc}{\protect\thispagestyle{empty}}
	\tableofcontents

	% ============================================ %
	% Вступ	
	\newpage
	\thispagestyle{empty}
	\addcontentsline{toc}{section}{Вступ}
	\section*{Вступ}
	\begin{center}\end{center}
	
	Оберненими задачами називають такі задачі, в яких необхідно відновити дані про деякий процес з використанням непрямих спостережень. Такі спостереження отримують за допомогою прямого процесу який, зазвичай, є необоротнім. Як наслідок, задачі такого типу можуть бути нерозв'язними або не мати єдиного розв'язку без додаткової інформації про об'єкт дослідження. До таких погано обумовлених задач можна віднести багато прикладів з реальних фізичних процесів, таких як задачі акустичної томографії або сейсморозвідки на основі звукових сигналів. Існує також широке поле обернених задач в області зображень та комп'ютерного бачення, таких як реконструкція рентгенівських знімків, видалення шуму, збільшення розмірності або заповнення втрачених даних у зображеннях та багато інших. 

	Класичні методи до розв'язання таких задач припускають наявність певної попередньої інформації про обернену задачу, на основі якої будується прямий оператор та функція регуляризації. В результаті, формується задача мінімізації, яка зводиться до знаходження розв'язку який одночасно добре підходить під наявні спостереження, а також, є ймовірним враховуючи попередні знання.
	
	Хоча такий підхід широко використовується, за останній час алгоритми глибокого навчання для розв'язування обернених задач набирають значну популярність через свою ефективність та універсальність для багатьох різних типів задач, що було продемонстровано в роботі Gregory Ongie та ін. \cite{ongie2020deep}. 
	
	В межах цієї роботи будемо розглядати деякі існуючі методи по реконструкції зображень на основі алгоритмів глибокого навчання, спробуємо проаналізуємо їх ефективність та порівняти з класичними методами.
	% ============================================ %
	
	\newpage
	\thispagestyle{empty}
	\section{Постановка задачі}
		
	Оберненими задачами будемо вважати такі задачі, в яких невідомим є $n-\text{піксельне}$ зображення $\boldsymbol{x} \in \mathbb{R}^{n}$ яке було отримане з $m$ вимірювань $\boldsymbol{y} \in \mathbb{R}^{m}$ відповідно до рівняння \ref{eq:forward-problem}.
	\begin{equation}
	\label{eq:forward-problem}
	\boldsymbol{y}=\mathcal{A}\left(\boldsymbol{x}\right)+\boldsymbol{\varepsilon}
	\end{equation}
	де $\mathcal{A}$ - це прямий оператор вимірювання та $\boldsymbol{\varepsilon}$ є певним вектором шуму. Метою задачі є відновлення $x$ з $y$. Можна розглянути більш загальний випадок моделі неадитивного шуму, який має вигляд \ref{eq:forward-problem-non-additive}.
	\begin{equation}
	\label{eq:forward-problem-non-additive}
	\boldsymbol{y}=\mathcal{N}\left(\mathcal{A}\left(\boldsymbol{x}\right)\right)
	\end{equation}
	де $\mathcal{N}(\cdot)$ є прикладами вибірки з шумом.


	\begin{defn}
		\label{def:well-posed}
		Відповідно до поняття, уведеного Жаком Адамаром, задачу \ref{eq:forward-problem-non-additive} називають коректно поставленою, якщо вона задовольняє наступні умови: 
		\begin{enumerate}
			\item Для кожного $x$ розв'язок задачі існує.
			\item Розв'язок є єдиний для кожного $x$.
			\item Розв'язок є стійкий до малих варіацій величини $x$, тобто достатньо малим зміненням величини $x$ відповідають як завгодно малі зміни величини $y$.
		\end{enumerate}
	\end{defn}

	\begin{defn}
		\label{def:ill-posed}	
		Задачу, яка не задовольняє хоча б одну з умов означення \ref{def:well-posed}, називають некоректно поставленою.
	\end{defn}

	Тому, очевидно, що розглянута обернена задача є некоректно (або погано обумовленою), оскільки в ній порушуються умови означення \ref{def:well-posed}. Така задача знаходження єдиного розв'язку, який задовольняє спостереженням, є складною або неможливою за умови відсутності попередніх знань про дані.

	Оцінку справжнього зображення $\boldsymbol{x}$ з $\boldsymbol{y}$  вимірювання називають задачею реконструкції зображення. Класичні підходи до реконструкції зображень припускають наявність деякої попередньої інформації про зображення, яку називають пріором. В якості пріору можуть виступати параметри гладкості, щільності та інші геометричні властивості зображення.

	Отже, метою даної роботи буде розв'язання таких обернених задач за допомогою алгоритмів глибокого навчання. Зокрема, будемо розглядати проблему видалення шуму у зображеннях.

	% ============================================ %
	\newpage
	\thispagestyle{empty}
	\section{Структура обернених задач}

	Відповідно до постановки задачі, ми прагнемо відновити векторизоване зображення $\boldsymbol{x} \in \mathbb{R}^{n}$ з вимірювань $\boldsymbol{y} \in \mathbb{R}^{m}$ у вигляді $\boldsymbol{y}=\mathcal{A}\left(\boldsymbol{x}\right)+\boldsymbol{\varepsilon}$.

	Якщо розподіл шуму відомий, $x$ можна відновити розв'язавши задачу оцінки максимальної ймовірності (maximum likelihood) \ref{eq:ML-problem}.
	\begin{equation}
		\label{eq:ML-problem}
		\hat{\boldsymbol{x}}_{\mathrm{ML}}
		= \arg \max_{\boldsymbol{x}} {p (\boldsymbol{y} | \boldsymbol{x})}
		= \arg \min_{\boldsymbol{x}} -\log p(\boldsymbol{y} | \boldsymbol{x})
	\end{equation}
	де $p(\boldsymbol{y} \mid \boldsymbol{x})$ це ймовірність спостереження $\boldsymbol{y}$ за умови якщо $\boldsymbol{x}$ є справжнім зображенням.
	
	В залежності від умов задачі, можуть бути відомі попередні дані про те яким має бути $x$. Ці умови можна використати для формулювання задачі оцінки максимальної апостеріорної ймовірності (maximum a posteriori), що приводить до задачі \ref{eq:MAP-problem}.
	\begin{equation}
		\label{eq:MAP-problem}
		\hat{\boldsymbol{x}}_{\mathrm{MAP}}
		=
		\arg \max_{\boldsymbol{x}} p(\boldsymbol{x} | \boldsymbol{y}) 
		=
		\arg \max_{\boldsymbol{x}} {p(\boldsymbol{y} | \boldsymbol{x})} p(\boldsymbol{x})
		=
		\arg \min_{\boldsymbol{x}} -\ln p(\boldsymbol{y} | \boldsymbol{x})-\ln p(\boldsymbol{x})
	\end{equation}
	
	Для випадку білого гаусівського шуму, цільову функцію можна сформулювати як:
	\begin{equation}
		\label{eq:MAP-avgn}
		\hat{x}=\arg \min_{x} 	\frac{1}{2}\|\mathcal{A}(\boldsymbol{x})-\boldsymbol{y}\|_{2}^{2}+\lambda \mathrm{R}(\boldsymbol{x})
	\end{equation}
	де  $|\mathcal{A}(\boldsymbol{x})-\boldsymbol{y}\|_{2}^{2}$ відповідає за правдивість даних та позначає різницю між вихідним та шумним зображеннями, $\mathrm{R}(\boldsymbol{x})$- від'ємний логарифмічний пріор який позначає член регуляризації, а $\lambda$ є параметром регуляризації. Для варіаційних методів видалення шуму, ключовим є пошук відповідного пріору зображення $\mathrm{R}(\boldsymbol{x})$. Варіантами таких пріорів моделі можуть бути градієнтні або розріджені пріори.

	Прикладом такого підходу до розв'язування некоректних задач є метод регуляризації Тіхонова. Він базується на мінімізації з використанням параметра регуляризації $\mathrm{R}_{\mathrm{TR}}$ за $L_2$ нормою, який можна подати у вигляді \ref{eq:tikhonov-reg-parameter}.
	
	\begin{equation}
		\label{eq:tikhonov-reg-parameter}
		\mathrm{R}_{\mathrm{TR}}(\boldsymbol{x})
		=
		\|\nabla \boldsymbol{x}\|_{2} 
		=
		\sqrt{\left|\nabla_{v} x\right|^{2}+\left|\nabla_{h} x\right|^{2}}
	\end{equation}
	де $\nabla_{h} \boldsymbol{x}$ та $\nabla_{v} \boldsymbol{x}$ є операторами градієнта по горизонталі та вертикалі зображення відповідно.
	
	%% cite https://www.hindawi.com/journals/am/2014/934834/	
	Задача максимальної апостеріорної оцінки може використовуватись для реконструкції зображень, однак такий підхід може бути не таким ефективним, якщо розподіл шуму або прямий оператор $\mathcal{A}$ є невідомі.  
	Алгоритми, основані на використанні машинного навчання, дають змогу побороти більшість з цих труднощів, що робить їх ефективною альтернативою класичному підходу.

	% ============================================ %
	\newpage
	\thispagestyle{empty}
	\section{Огляд глибокого навчання для обернених задач}

	\subsection{Контрольоване і неконтрольоване навчання}
	Перший і найпоширеніший тип розв'язування обернених задач з використанням глибокого навчання є контрольована інверсія. Ідея полягає у створенні співвідношення між датасетом справжніх зображень $x$ та відповідними вимірюваннями $y$. Тобто ми можемо натренувати нейронну мережу приймати значення $y$ та реконструювати оберенне значення $x$. Цей підхід є доволі ефективним, однак є чутливим до змін в операторі вимірювання $\mathcal{A}$. 
	
	Другим типом розв'язування обернених задач є неконтрольованого навчання. Він передбачає, що інформація про пари вхідної та вихідної інформації $x$ та $y$ невідомі під час тренування. До нього можна віднести ситуації, коли відомі тільки справжні зображення $x$ або тільки результати вимірювання $y$.
	
	Ці два підходи мають фундаментальні відмінності і ця робота націлена саме на методи контрольованого навчання, тому що очікується, що вони дадуть кращі результати в порівнянні з класичними методами. 
	
	\subsection{Класифікація глибокого навчання для обернених задач}

	В роботі \cite{ongie2020deep} наведена детальна класифікація основної кількості існуючих підходів до розв'язання обернених задач на основі глибокого навчання. 	Більшість з них можна поділити на декілька груп:
	\begin{itemize}
		\item \textbf{Пряма модель $\mathcal{A}$ є відома під час тренування та тестування} \newline
		Для цього випадку найбільш доцільним підходом є використання контрольованих моделей машинного навчання, тому що маючи доступ до оригінальних зображень та прямого оператора, можна легко згенерувати пари для тренування моделі. До прикладів можна віднести задачу рентгенівських перетворень в комп’ютерній томографії.
		
		\item \textbf{Пряма модель $\mathcal{A}$ є відома тільки під час тестування} \newline
		Такі алгоритми корисні для навчання моделей загального призначення. Якщо один раз навчити глибоку модель, цю ж саму модель можна буде використовувати для будь-якої іншої прямої моделі. Це вигідно в ситуаціях, коли є достатня кількість чистих зображень, а навчати глибокі нейронні мережі для різних прямих моделей є недоцільно.
			
		\newpage
		\item \textbf{Пряма модель $\mathcal{A}$ є відома тільки частково} \newline
		Така ситуація можлива, наприклад, у випадку коли пряма модель залежить від деяких параметрів, для яких ми знаємо або розподіл, або достатню статистику.

		\item \textbf{Пряма модель $\mathcal{A}$ є невідома} \newline
		У деяких випадках пряма-модель може бути абсолютно невідомою, неправильно визначеною або обчислювально неможливою для використання в навчанні та тестуванні. Тоді навчання може відбуватись лише з відповідними парами зображень та вимірювань. 
	\end{itemize}
	
	\newpage
	\thispagestyle{empty}
	\section{Автоенкодер для розв'язування обернених задач}

	Вперше автоенкодер був представлений в роботі \cite{10.5555/104279} як нейронна мережа яка тренується відтворювати свої вхідні дані. З того часу він використовувався в багатьох задачах з області машинного навчання та був детально описаний Ian Goodfellow та ін. в \cite{Goodfellow-et-al-2016}. Зокрема, автоенкодер є досить ефективною моделлю для розв'язування обернених задач.
	
	\subsection{Автоенкодер}
	
	Автоенкодером називають нейрону мережу яка навчається копіювати свої вхідні дані у вихідні. Така мережа має проміжний шар $h$ або "код"\, до якого будемо звертатись як до репрезентаційного. Він зберігає параметри, необхідні для представлення вхідних даних. Таку нейрону мережу можна подати у складі двох частин: функції енкодера $h = f(x)$ та декодера який відтворює $r = g(h)$. Ця архітектура подана на зображенні \ref*{fig:autoencoder-graph}.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.9\textwidth]{../resources/autoencoder.pdf}
		\caption{Загальна структура автокодера, що відображає вхід $x$ - "оригінальне зображення"\ на вихід $r$ - "відтворене зображення"\ через внутрішнє представлення $h$ - "код". Автоенкодер складається з двох компонент: енкодера $f$ (відображення $x$ до $h$) та декодера $g$ (відображення $h$ до $r$).}
		\label{fig:autoencoder-graph}
	\end{figure}
	Якщо автоенкодеру вдається навчитися просто відтворювати $g(f(x)) = x$ для всіх прикладів, то це, зазвичай, не має особливої користі. Тому автоенкодери часто обмежують таким чином, щоб вони не могли відтворювати ідеальну копію вхідних даних. Для цього можна, наприклад, регулювати розмірність репрезентаційного шару таким чином, щоб вона була суттєво меншою за розмірність вхідних даних.
	
	\newpage
	\subsection{Автоенкодер для видалення шуму}
	Класичні автоенкодери мінімізують деяку функцію:
	\begin{equation}
		L(\boldsymbol{x}, g(f(\boldsymbol{x})))
	\end{equation}
	де $L$ це штрафна функція яка визначає відмінність функції $g(f(\boldsymbol{x}))$ від $\boldsymbol{x}$, таку як, наприклад,  $L^{2}$ норма від їх різниці. В результаті це призводить до того, що композиція функцій $g \circ f$ навчається бути тотожнім відображенням якщо для того є можливість. На відміну від цього, автоенкодер для видалення шуму мінімізує:
	\begin{equation}
		L(\boldsymbol{x}, g(f(\tilde{\boldsymbol{x}})))
	\end{equation}
	де $\tilde{\boldsymbol{x}}$ є копією $\boldsymbol{x}$ який був пошкоджений деяким шумом. 
	
	Отже, такий автоенкодер має не просто відтворити вхідні дані, а ще й відновити пошкодження. Процес тренування автоенкодера для видалення шуму заданий на \ref{fig:dae-graph}.
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.9\textwidth]{../resources/dae.pdf}
		\caption{
			Структура функції витрат для автоенкодера який навчається реконструювати оригінальні зображення $x$ з пошкоджених деяким випадковим шумом. Тренування виконується на основі мінімізації функції втрат при реконструкції, яку позначимо як: $L = - \log p (\boldsymbol {x} | \boldsymbol {h} = f (\tilde {\boldsymbol {x}}))$, де $\tilde {\boldsymbol {x}}$ пошкоджена версія прикладу $\boldsymbol {x}$, отримана в результаті деякого процесу руйнування.
		}
		\label{fig:dae-graph}
	\end{figure}
	Такий підхід до навчання видалення шуму змушує $f$ та $g$ явно вивчати структуру даних, що дозволяє йому ефективно видаляти шум з пошкоджених зображень.
	
	Автоенкодер для видалення шуму доцільно використовувати у випадку коли пряма модель $\mathcal{A}$ та чисті зображення є відомі, або коли є достатня кількість пар чистих та пошкоджених зображень для тренування нейронної мережі. 
		
	% ============================================ %		
	\newpage
	\thispagestyle{empty}
	\section{Модель глибокої нейронної мережі}

	Нехай маємо набір тренувальних даних:
	\begin{equation*}
		(x^{(1)}, y^{(1)}), \quad (x^{(2)}, y^{(2)}), \quad ... \quad ,(x^{(m)}, y^{(m)})
	\end{equation*}
	\begin{equation*}
		\quad
		x =	
		\begin{bmatrix}
			\vdots  & \vdots  & & \vdots \\
			x^{(1)} & x^{(2)} &   \dots & x^{(m)}\\
			\vdots  & \vdots  & & \vdots
		\end{bmatrix}
		\quad
		x\ith \in
		\begin{bmatrix}
			x\ith_1   \\
			\vdots \\
			x\ith_n
		\end{bmatrix}
		\quad
	\end{equation*}
	\begin{equation*}
		\quad
		y =	
		\begin{bmatrix}
			\vdots  & \vdots  & & \vdots \\
			y^{(1)} & y^{(2)} &   \dots & y^{(m)}\\
			\vdots  & \vdots  & & \vdots
		\end{bmatrix}
		\quad
		y\ith \in
		\begin{bmatrix}
			y\ith_1   \\
			\vdots \\
			y\ith_n
		\end{bmatrix}
		\quad
	\end{equation*}
	де
	\begin{itemize}
		\item $x$ - матриця в якій кожен $i-\text{ий}$ стовпець є розгорнутим у вектор чистим зображенням, $i=1,\dots,m$
		\item $y$ - матриця в якії кожен $i-\text{ий}$ стовпець є розгорнутим у вектор пошкодженим зображенням, $i=1,\dots,m$
		\item $m$ - кількість прикладів
		\item $n$ - кількість характеристик в кожному прикладі (довжина розгорнутого в вектор зображення)
	\end{itemize}

	\subsection{Пряме поширення}
	Алгоритм прямого поширення для глибокої нейронної мережі має вигляд \ref{eq:dnn-forward-propagation}.
	\begin{equation}
		\label{eq:dnn-forward-propagation}
		\begin{array}{l}
			\displaystyle
			z\lth=w^{(l)} a^{(l-1)}+b\lth
			\\[0.7cm]
			
			\displaystyle
			a\lth=\sigma (z\lth)
		\end{array}
	\end{equation}
	де
	\begin{itemize}
		\item $l$ - номер шару нейронної мережі, при $l = 1 \dotsc L$
		\item $n^{[l]}$ - кількість нейронів в $l$ шарі
		\item $a\lth$ - вектор стовпець активацій нейронів на для шару $l$ $(n^{[l]}$ $\cross$ 1)
		\item $b\lth$ - вектор стовпець ваг зміщення $(n^{[l]}$ $\cross$ 1)
		\item $w\lth$ - матриця ваг поміж шарами $l-1$ та $l$ $(n^{[l]} \cross n^{[l - 1]})$ 
		\item $\sigma$ - це деяка активаційна (стискуюча) функція, яку ми можемо прийняти як логістичну (для діапазону від 0 до 1) або tanh (для діапазону від -1 до 1), або будь-яку іншу диференційовану функцію.
	\end{itemize}
	
	Визначимо штрафну функцію \ref{eq:cost-function} як середньоквадратичну похибку між активаціями останнього шару $a^{(L)}$ та справжніми зображеннями $y$.
	\begin{equation}
		\label{eq:cost-function}
		J(\omega, b) =  \frac{1}{m}  \sum_{i=1}^{m} L(a^{(L, i)}, y\ith)
	\end{equation}
	де, функцією втрати для одного набору елементів визначимо половину евклідової відстані \ref{eq:loss-function}.
	\begin{equation}
		\label{eq:loss-function}
		L(a^{(l, i)}, y\ith)  =  \frac{1}{2}  \| a^{(l, i)}  - y\ith \|_{L_2}^{2} = \frac{1}{2} \sum_{j=1}^{n}  (a^{(l, i)}_j -  y\ith_j)^2
	\end{equation}
	де $\| \cdot \|$ це $L_2$ норма.

	\subsection{Зворотнє поширення}

	Задача полягає в тому, щоб знайти параметри $w \in \mathbb{R}^{m}, b\in \mathbb{R}$ що мінімізують штрафну функції реконструкції:
	\begin{equation}
		\arg \min_{w, b} J (w, b)
	\end{equation}
	Мінімізацію будемо проводити алгоритмом градієнтного спуску. Для цього, спочатку обчислюємо похідну від функції середньоквадратичної похибки останнього шару нейронної мережі.
	\begin{equation}
		\begin{array}{l}
			\displaystyle
			da^{(L, i)}
			=
			\pdv{L(a^{(L, i)}, y\ith)}{a^{(L, i)}}
			=
			\sum_{j=1}^{n}{(a^{(L, i)}_j - y\ith_j)}
			\\[0.7cm]

			\displaystyle
			dz^{(L, i)}
			=
			\pdv{L(a^{(L, i)}, y\ith)}{z^{(L, i)}}
			= 
			\pdv{L(a^{(L, i)}, y\ith)}{a^{(L, i)}} \pdv{a^{(L, i)}}{z^{(L, i)}}
			=
			\sum_{j=1}^{n}{(a^{(L, i)}_j - y\ith_j)} \sigma^{\prime}(z^{(L, i)});
			\\[0.7cm]

			\displaystyle
			dw^{(L, i)} 
			=
			\pdv{L(a^{(L, i)}, y\ith)}{w^{(L, i)}}
			=
			dz^{(L, i)} \pdv{z^{(L, i)}}{w^{(L, i)}}
			=
			dz^{(L, i)} a^{(L-1, i)};
			\\[0.7cm]
	
			\displaystyle
			db^{(L, i)}
			=
			\pdv{L(a^{(L, i)}, y\ith)}{b^{(L, i)}}
			=
			dz^{(L, i)} \pdv{z^{(L, i)}}{b^{(L, i)}} 
			= 
			dz^{(L, i)}
			\\[0.7cm]
		\end{array}
	\end{equation}

	Перепишемо похідні в більш компактному вигляді, для всіх шарів нейронної мережі:
	\begin{equation}
		\begin{array}{l}
			\displaystyle
			\frac{\partial J(w, b)}{\partial W^{(l)}} =\frac{1}{m} \sum_{i}^{m} 	\delta^{(l, i)}\left(a^{(l-1, i)}\right)^{\top}
			\\[0.7cm]
	
			\displaystyle
			 \frac{\partial J(w, b)}{\partial b^{(l)}} =\frac{1}{m} \sum_{i}^{m} \delta^{(l, i)}
		\end{array}
	\end{equation}
	де
	\begin{equation}
		\begin{array}{l}
		\displaystyle
			\delta^{(l, i)}
			=
			\left\{
			\begin{array}{l}
				\displaystyle
				\sum_{j=1}^{n}{ \left(a^{(L, i)}_j - y\ith_j\right)} \sigma^{\prime}(z^{(L, i)}), \quad \text{якщо } l = L
				\\[0.7cm]
				
				\displaystyle
				\left((W^{(l)})^{\top} \delta^{(l+1, i)} \right) \sigma^{\prime} (z^{(l, i)}), \quad \text{якщо } l < L
			\end{array}\right.
		\end{array}
	\end{equation}

	В залежності від активаційної функції визначеної на певному шарі нейронної мережі, значення похідної буде відрізнятись. Розглянемо наступні варіанти активаційних функцій, які будемо застосовувати далі:
	\begin{itemize}	
		\item \textbf{Sigmoid} (Logistic function)
		\label{eq:sigmoid}
		\begin{equation}
			\begin{array}{l}
				\displaystyle
				\sigma(z)=\frac{1}{1+e^{-z}} \\[0.5cm]

				\displaystyle
				\sigma^{\prime}(z)=a(1-a)
			\end{array}
		\end{equation}
		
		\item \textbf{ReLU} (Rectiﬁed Linear Units)
		\begin{equation}
			\label{eq:relu}
			\begin{array}{l}
				\displaystyle
				\sigma(z)=\max (0, z) \\[0.5cm]

				\displaystyle
				\sigma^{\prime}(z)=\left\{\begin{array}{l}
					0, z<0 \\
					1, z \geq 0
				\end{array}\right.
			\end{array}
		\end{equation}
		
		\item \textbf{Tanh} (Hyperbolic tangent)
		\begin{equation}
			\label{eq:tanh}
			\begin{array}{l}
				\displaystyle
				\sigma(z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}} \\[0.5cm]
				
				\displaystyle
				\sigma^{\prime}(z)=1-a^{2}
			\end{array}
		\end{equation}
	\end{itemize}
		
	Тепер можемо записати алгоритм зворотного поширення з використанням стохастичного градієнтного спуску та minibatch оптимізації \ref{alg:gradient-descent}.
	\begin{algorithm}[H]
	\caption{Градієнтний спуск}
	\label{alg:gradient-descent}
	\begin{algorithmic}[1]
		\State \algorithmicrequire{Тренувальні дані $x, y$, гіперпараметри: $N$, $M$, $\varepsilon$, $\alpha$}
		\State \algorithmicensure{ Оптимальні параметри моделі $w, b$}
		\State $i = 0$
		\While{ i $< N$ or ( $\|d\omega\| < \varepsilon$ and $\|db\| < \varepsilon$ )}
		\State $x^M \leftarrow$ випадкова група прикладів $x$, розміром $M$
		\State $y^M \leftarrow$ випадкова група прикладів $y$, розміром $M$
		\For{ $l = 1$ to $L$ }
		\State $\displaystyle w^{(l)} = w^{(l)} -  \alpha \pdv{J(w, b, x^M, y^M)}{w^{(l)}}$
		\vspace{0.2cm}
		\State $\displaystyle b^{(l)} = b^{(l)} - \alpha \pdv{J(w, b, x^M, y^M)}{b^{(l)}}$
		\State $\displaystyle i = i + 1$
		\EndFor
		\EndWhile
		\State \algorithmreturn{$w, b$}.
	\end{algorithmic}
	\end{algorithm}

	% ============================================ %
	\newpage
	\thispagestyle{empty}
	\section{Генерація та оцінка шуму в зображеннях}
	
	\subsection{Генерація шуму}	
	Для того, щоб застосувати описані методи видалення шуму, необхідно спочатку визначити яким чином цей шум, тобто прямий оператор $\mathcal{A}$ вибрати. Для цього будемо використовувати адитивний білий гаусівський шум з різними параметрами стандартного відхилення. Задамо загальну модель адитивного шуму як \ref{eq:additive-noise}.
	\begin{equation}
		\label{eq:additive-noise}
		\mathbf{y}=\mathbf{x} + \mathbf{\varepsilon}
	\end{equation}
	де $\varepsilon$ це шум який має нульове середнє значення та відповідає нормальному розподілу Гауса:
	\begin{equation}
		\varepsilon_{i} \sim \mathcal{N}\left(0, \sigma^{2}\right) .
	\end{equation}
	
	Ми можемо змоделювати чисте від шумів зображення $\mathbf{x} \in \mathbb{R}^{т}$ як розподіл Гауса з нульовою дисперсією, тобто $\mathbf{x}_{i} \sim \mathcal{N}(\mathbf{x}, 0)$, що дає нам змогу подати $\mathbf{y}$ теж як розподіл Гауса. Сума двох розподілів Гауса $y_{1} \sim \mathcal{N}\left(\mu_{1}, \sigma_{1}^{2}\right)$ та $y_{2} \sim \mathcal{N}\left(\mu_{2}, \sigma_{2}^{2}\right)$ також є розподілом Гауса $y_{1}+y_{2} \sim \mathcal{N}\left(\mu_{1}+\mu_{2}, \sigma_{1}^{2}+\sigma_{2}^{2}\right)$. Таким чином, шумні  вимірювання $y_{i}$ будуть відповідати по-піксельному нормальному розподілу:
	\begin{equation}
		p\left(\mathbf{y}_{i} \mid \mathbf{x}_{i}, 		\sigma\right)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{\left(\mathbf{y}_{i}-\mathbf{x}_{i}\right)^{2}}{2 \sigma^{2}}}
	\end{equation}
	
	де $i = 1\dotsc n$ та $n$ - кількість пікселів. Після додавання шуму до зображення необхідно також виконати по-піксельне обрізання так, щоб$x_i \in [0, 1]$, тобто значення не виходили за межі оригінального зображення.
	%Більше про шум:
	%https://stanford.edu/class/ee367/reading/lecture10_notes.pdf
	
	\subsection{Оцінка зображень}
	Для аналізу отриманих результатів необхідно мати метрики того, наскільки зображення, оброблені алгоритмом видалення шуму,  відрізняються від оригінальних зображень. Очевидним варіантом є використання середньо квадратичної похибки, яка також використовується у штрафній функції нейронної мережі.
	\begin{equation}
		M S E(x, y) =\frac{1}{n^2} \sum_{i=0}^{n-1} \sum_{j=0}^{n-1}[x(i, j)-y(i, j)]^{2}
	\end{equation}
	де $x$ та $y$ відповідає зображеннями розміром $n \cross n$.
	
	Однак, зазвичай, така метрика є не найкращим відображення людського сприйняття зображень. Більш об'єктивною альтернативою є SSIM (structural similarity index measure) метрика яка була представлена в роботі \cite{1284395}. Вона дає можливість краще оцінити схожість двох зображень $x$ та $y$ опираючись на різницю в структурі всього зображення, а не окремих пікселів. SSIM метрика задається формулою \ref{eq:SSIM}.
	\begin{equation}
		\label{eq:SSIM}
		\operatorname{SSIM}(x, y)=\frac{\left(2 \mu_{x} \mu_{y}+c_{1}\right)\left(2 \sigma_{x 	y}+c_{2}\right)}{\left(\mu_{x}^{2}+\mu_{y}^{2}+c_{1}\right)\left(\sigma_{x}^{2}+\sigma_{y}^{2}+c_{2}\right)}
	\end{equation}
	де
	\begin{itemize}
		\item $\mu_{x}$ середнє значення $x$
		\item $\mu_{y}$ середнє значення $y$
		\item $\sigma_{x}^{2}$ дисперсія $x$
		\item $\sigma_{y}^{2}$ дисперсія  $y$
		\item $\sigma_{x y}$ коваріація $x$ та $y$
		\item $c_{1}=\left(k_{1} L\right)^{2}, c_{2}=\left(k_{2} L\right)^{2}$ змінні для стабілізації ділення
		\item $L$ динамічний діапазон пікселів
		\item $k_{1}=0.01$ та $k_{2}=0.03$ - константи.	
	\end{itemize}
	
	Значення цієї функції змінюється в діапазоні $[-1, 1]$, де $1$ можна отримати у випадку коли зображення однакові.
	
	% ============================================ %	
	\newpage
	\thispagestyle{empty}
	\section{Реалізація моделі для автоенкодера}
	
	\subsection{Датасет}
	
	Тренувати модель будемо використовуючи MNIST датасет який складається з 70 тис. тренувальних рукописних цифр. Всі зображення є чорно-білими та розмір кожного із них складає $28 \times 28$ пікселів, а значення лежать в проміжку від $[0,255]$. Для пришвидшення тренування дані  були нормалізовані до діапазону $[0, 1]$. Весь датасет був поділений на частину тренувальних та тестових прикладів в пропорції 90/10.
	
	\subsection{Архітектура автоенкодера}	
	
	Для роботи автоенкодера зазвичай достатньо мати по одному шару енкодера та декодера. Однак, глибокі автоенкодери мають додаткові переваги та можуть вивчати складніші взаємозв'язки даних. Можна збільшувати як глибину енкодера для покращення представлення даних, так і декодера для покращення їх відтворення. Однак, збільшення кількості шарів нейронної мережі може суттєво вплинути на час її тренування і тому необхідно вибирати їх оптимальну кількість.
	
	Важливим є також вибір розмірності кожного з шарів автоенкодера. Вхідний шар завжди має розмірність зображення, тобто для випаду нашого датасету це 784. Кількість нейронів у вихідному шарі має відповідати кількості нейронів вхідного, тому що ми прагнемо відновити чисте зображення. Для прихованих шарів автоенкодера, зазвичай вибирають розмір між значеннями вхідного та вихідного шарів. 
	
	Ключовим гіперпараметром автоеркодера можна вважати розмір репрезентаційного шару. Від нього залежить наскільки енкодер буде компресувати вхідну інформацію, що прямо впливає на можливість її подальшого відтворення. Очевидно, що при малому розмірі цього шару модель декодер буде мати менше деталей для відтворення зображення, і як наслідок, буде давати гірші результати.  Однак, якщо зробити його розмірність занадто великою автоенкодер може просто навчитись копіювати свої вхідні дані у вихідні, не вивчаючи властивостей даних. Такий автоенкодер буде точно реконструювати навчальні дані, але через перетренованість він не зможе коректно працювати з новими прикладами.
	
	Таким чином, відповідно до наведених вище критеріїв була побудована наступна архітектура моделі автоенкодера, яка задана таблицею \ref{tab:autoencoder-model}.
	
	\newpage
	\begin{center}
		\begin{table}[!htbp]
			\centering
			\begin{tabular}{|c|c|}
				\hline \tabboxc{10cm}{Шар мережі (активаційна функція)}
				& \tabboxc{4cm}{Розмірність} \\
				
				\hline \multicolumn{2}{|c|}{\tabboxc{2cm}{Енкодер}} \\
				
				\hline \tabboxc{4cm}{Dense (Relu)}
				& $784 \times 64$ \\
				
				\hline \tabboxc{4cm}{Dense (Relu)}
				& $64\times 32$ \\
				
				\hline \multicolumn{2}{|c|}{\tabboxc{2cm}{Декодер}} \\
				
				\hline \tabboxc{4cm}{Dense (Sigmoid)}
				& \tabboxc{3cm}{$32\times784$}\\
				\hline
			\end{tabular} 
			\caption{Архітектура щільної нейронної мережі для автоенкодера.}
			\label{tab:autoencoder-model}
		\end{table}
	\end{center}
	Як можна побачити для енкодера було вибрано два шари, а для декодера один. Така модель була вибрана емпірично і як можна буде побачити далі, вона демонструє досить непогані результати по відтворенню зображень. Репрезентаційний шар складається з 32 нейронів що виявилось оптимальним значенням за критерієм точності відтворення даних та часу витраченого на тренування.
	
	\subsection{Тренування}
	Всього були натреновані п'ять моделей для різних значень стандартного відхилення білого гаусівського шуму. Для тренування кожної з них використовувався однаковий набір гіперпараметрів заданий в таблиці \ref{tab:model-hyperparameters}.
	\begin{center}
		\begin{table}[!htbp]
			\centering
			\begin{tabular}{|c|c|c|}
				\hline \tabboxc{5cm}{Кількість ітерацій}
				& \tabboxc{5cm}{Розмір minibatch частини}
				& \tabboxc{5cm}{Швидкість навчання} \\
				
				\hline \tabboxc{5cm}{35}
				& \tabboxc{5cm}{128}
				& \tabboxc{5cm}{0.015} \\
				\hline
			\end{tabular} 
			\label{tab:model-hyperparameters}
		\end{table}
	\end{center}

	Окрім звичайної штрафної функції, під час тренування також використовувалась SSIM оцінка для аналізу ефективності відтворення зображень на тестових даних, що продемонстровано на графіку \ref{fig:awgn-train-ssim-comparation}.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.9\textwidth]{../resources/awgn-train-ssim-comparation.pdf}
		\caption{Графік залежності усередненої SSIM оцінки для тестового датасету від кількості ітерацій тренування. $\sigma$ відповідає середньоквадратичному відхиленню гаусівського шуму.}
		\label{fig:awgn-train-ssim-comparation}
	\end{figure}
	
	Для порівняння був також розглянутий варіант коли $\sigma = 0$. В такому випадку шум у зображення не додавався. Як можна бачити, існує чітка залежність між рівнем шуму доданим до чистих зображень та ефективністю його відтворення автоенкодером. Для $\sigma = 0$, середнє значення SSIM оцінки прямує до 0.8, коли для $\sigma = 1$ воно залишається в околі $0.1$. Попри низьку середню оцінку для високих показників шуму, автоенкодер все одно може давати непогані результати, як можна буде побачити далі.
	% ============================================ %	
		
	\newpage
	\thispagestyle{empty}
	\section{Аналіз результатів}
	
	Результати натренованих моделей автоенкодера для видалення шуму можна бачити на зображенні \ref{fig:denoising-awgn-comparation}.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=1\textwidth]{../resources/denoising-awgn-comparation.pdf}
		\caption{Порівняння точності реконструкції зображень автоенкодером для різної величини стандартного відхилення $\sigma$ білого шуму Гауса.}
		\label{fig:denoising-awgn-comparation}
	\end{figure}
	
	Очевидно, що при збільшенні кількості шуму, відтворенні зображення втрачають в чіткості та схожості з оригіналом. Зауважимо, що залежність MSE похибки від $\sigma$ є прямою, а для SSIM оцінки навпаки - оберненою. 
	
	Можна сказати, що навіть при дуже значній кількості шуму як $\sigma=1$, відтворене зображення все одно залишається читабельним, на відмінно від пошкодженого варіанту. З цього можна зробити висновок, що автоенкодер досить ефективно справляється з задачею по видаленню шуму, при достатній кількості тренувальних прикладів.
	
	Для порівняння ефективності роботи автоенкодера з класичними методами, був також реалізований алгоритм регуляризації Тіхонова, де як регуляризатор використовувався градієнтний пріор. Таким чином, ми порівнюємо між собою результати роботи обидвох алгоритмів для видалення шуму, заданого стандартним відхиленням $\sigma=0.5$. Порівняння продемонстровано на зображенні \ref{fig:denoising-methods-comparation} . 

	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\textwidth]{../resources/denoising-methods-comparation.pdf}
		\caption{Порівняння видалення шуму за допомогою автоенкодера з класичним методом основаним на регуляризації.}
		\label{fig:denoising-methods-comparation}
	\end{figure}

	Метод регуляризації суттєво покращив середньоквадратичну похибку, але загалом зображення залишилось сильно пошкодженим. З іншого боку, зображення відновленні за допомогою глибокої нейронної мережі демонструють непогані результати для обох наведених метрик. З цього виплаває, що реалізована модель глибокої нейронної мережі є більш ефективним підходом до розв'язання поставленої задачі по видаленню шуму.

	% ============================================ %
	\newpage
	\thispagestyle{empty}
	\addcontentsline{toc}{section}{Висновок}
	\section*{Висновок}
	\begin{center}\end{center}
	В ході цієї роботи ми розглянули деякі типи обернених задач та підходи до їх розв'язання. Спершу була сформульована постановка обернених задач та описана їх загальна структура. Для проведення практичних досліджень, з широкого спектра обернених задач була вибрана задача по видаленню шуму у зображення.
	
	Для поставленої задачі необхідно було вибрати спосіб додавати штучно згенерованих пошкоджень до зображень. Для цього був використаний білий гаусівський шум, який хоч і не завжди відповідає характеру пошкоджень в справжніх зображеннях, але часто використовується для їх імітації. Окрім того, були розглянуті деякі варіанти оцінювання пошкоджень у зображеннях.
	
	Для розв'язування оберненої задачі по видаленню шуму в зображеннях були описані та реалізовані декілька методів. Перший метод оснований на алгоритмі глибокого навчання та передбачає побудову нейронної мережі для автоенкодера. Другий підхід базується на ідеї класичної регуляризацій для задачі максимальної апостеріорної ймовірності. 
	
	За результатами експериментів можна сказати, що побудована модель глибокого навчання є досить ефективною у розв'язанні розглянутої задачі. Вона успішно впоралась з видаленням шуму при його низьких показниках та дала прийнятні результати для його надмірної кількості. Варто зазначити, що автоенкодер продемонстрував себе значно краще в порівнянні з методом регуляризації. Такий результат можна вважати очікуваним, адже він був натренований на значній кількості прикладів зображень, коли для регуляризації може бути достатньо тільки інформації про розподіл шуму.
	
	Підсумовуючи вищенаведене, можна стверджувати що використання глибокого навчання для розв'язання поставленої задачі є ефективним підходом до розв'язання обернених задач по видаленню шуму. Однак попри свою ефективність він не є універсальним і потребує тренування для кожної окремої задачі, а для його застосування необхідно мати достатню кількість тренувальних пар чистих та пошкоджених зображень. 
	
	На останок можна додати, що в області обернених задач існують також і багато інших проблем. Через природу таких задач, вони майже ніколи не можуть розв'язуватись точно, через що вони й надалі залишаються актуальними, а отже в цій області існує ще широке поле для подальших досліджень.

	\newpage
	\thispagestyle{empty}
	\addcontentsline{toc}{section}{Додатки}
	\section*{Додатки}

	\begin{figure}[H]
		\centering
		\includegraphics[width=1\textwidth]{../resources/autoencoder-denoising-samples.pdf}
		\caption{Приклад видалення шуму за допомогою автоенкодера. Перший ряд це чисті зображення, другий - пошкодженні, третій - відновленні. $\sigma=0.5$.}
		\label{fig:autoencoder-denoising-samples}
	\end{figure}	
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{../resources/awgn-train-loss-comparation.pdf}
		\caption{Графік штрафної функції MSE для тренувального датасету від кількості ітерацій тренування. $\sigma$ відповідає середньоквадратичному відхиленню гаусівського шуму.}
		\label{fig:awgn-loss-ssim-comparation}
	\end{figure}
		
	%============================================ %	

	\newpage
	\thispagestyle{empty}
	\addcontentsline{toc}{section}{Література}
	\printbibliography[title={Література}]

	% ============================================ %
\end{document}